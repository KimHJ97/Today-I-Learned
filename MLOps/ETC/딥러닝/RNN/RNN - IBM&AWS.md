# RNN - IBM

 - https://www.ibm.com/kr-ko/topics/recurrent-neural-networks
 - https://aws.amazon.com/ko/what-is/recurrent-neural-network/

<br/>

## 1. 순환 신경망이란?

순환 신경망(RNN)은 순차 데이터나 시계열 데이터를 이용하는 인공 신경망 유형입니다. 이 딥러닝 알고리즘은 언어 변환, 자연어 처리(nlp), 음성 인식, 이미지 캡션과 같은 순서 문제나 시간 문제에 흔히 사용됩니다. 그리고 Siri, 음성 검색, Google 번역과 같이 널리 쓰이는 애플리케이션에도 통합되어 있습니다. 피드포워드 및 컨볼루션 신경망(CNN)처럼 순환 신경망은 학습하는 데 훈련 데이터를 활용합니다. 이 신경망은 "메모리"에 의해 구별됩니다. 과거의 입력으로부터 정보를 얻어 현재의 입력과 출력에 영향을 주기 때문입니다. 기존의 딥 신경망에서는 입력과 출력이 상호 독립적이라고 가정하지만, 순환 신경망의 출력은 시퀀스 내의 이전 요소에 의존합니다. 미래의 이벤트도 지정된 시퀀스의 출력을 결정하는 데 도움이 되지만, 단방향의 순환 신경망의 경우 예측에서 이러한 이벤트를 설명할 수 없습니다.  

<br/>

## 2. 순환 신경망 유형

RNN은 일대일 아키텍처를 특징으로 하는 경우가 많습니다. 즉, 하나의 입력 시퀀스가 하나의 출력과 연결됩니다. 하지만 구체적인 용도에 따라 다양한 구성으로 유연하게 조정할 수 있습니다. 다음은 몇 가지 일반적인 RNN 유형입니다.

### 일대다
이 RNN 유형은 하나의 입력을 여러 출력으로 채널링합니다. 단일 키워드로 문장을 생성하여 이미지 캡션과 같은 언어 애플리케이션을 지원합니다.  

### 다대다
이 모델은 다중 입력을 사용하여 다중 출력을 예측합니다. 예를 들어 문장을 분석하고 다른 언어의 단어를 올바르게 구조화하는 RNN을 사용하여 언어 번역기를 만들 수 있습니다.  

### 다대일
여러 입력이 출력에 매핑됩니다. 따라서 모델이 입력된 평가로부터 고객의 감정을 긍정적, 부정적, 중립으로 예측하는 감성 분석과 같은 애플리케이션에 유용합니다.  

<br/>

## 3. 다른 신경망 아키텍처와 차이점

RNN은 다양한 신경망 아키텍처 중 하나입니다.  

### 순환 신경망 vs. 피드포워드 신경망
피드포워드 신경망은 RNN과 마찬가지로 아키텍처의 한쪽 끝에서 다른 쪽 끝으로 정보를 전달하는 인공 신경망입니다. 피드포워드 신경망은 간단한 분류, 회귀 또는 인식 작업을 수행할 수 있지만, 이전에 처리한 입력은 기억하지 못합니다. 예를 들어 뉴런에서 is라는 단어를 처리할 때쯤이면 Apple은 잊어버립니다. RNN은 숨겨진 메모리 상태를 뉴런에 포함하여 이러한 메모리 한계를 극복합니다.

<br/>

### 순환 신경망 vs. 컨벌루션 신경망
컨벌루션 신경망은 시간 데이터를 처리하도록 설계된 인공 신경망입니다. 컨벌루션 신경망을 사용하면 비디오와 이미지를 신경망의 일련의 컨벌루션 계층과 풀링 계층에 전달하여 공간 정보를 추출할 수 있습니다. RNN은 순차 데이터의 장기 종속성을 캡처하도록 설계되었습니다.

<br/>

## 4. 변종 RNN 아키텍처

### 양방향 순환 신경망(BRNN)
RNN의 변종 네트워크 아키텍처 중 하나입니다. 단방향 RNN은 현재 상태에 관해 예측하기 위해 과거의 입력에서 가져오는 것만 가능하지만, 양방향 RNN은 정확도를 높이기 위해 미래의 데이터를 가져옵니다. 앞서 나왔던 “feeling under the weather”의 예에서는 모델이 시퀀스의 마지막 단어가 "weather"임을 알면 2번째 단어가 “under”임을 더 확실히 예측할 수 있습니다.  

양방향 순환 신경망(BRNN)은 숨겨진 노드의 순방향 및 역방향 계층으로 데이터 시퀀스를 처리합니다. 순방향 계층은 이전 입력을 숨겨진 상태로 저장하고, 이를 후속 출력을 예측하는 데 사용하는 RNN과 유사하게 작동합니다. 반면, 역방향 계층은 현재 입력과 미래의 숨겨진 상태를 모두 가져와 현재 숨겨진 상태를 업데이트함으로써 반대 방향으로 작동합니다. BRNN은 두 계층을 결합함으로써 과거와 미래의 컨텍스트를 고려하여 예측 정확도를 높일 수 있습니다. 예를 들어 BRNN을 사용하여 사과나무는 키가 크다는 문장에서 나무라는 단어를 예측할 수 있습니다.  

<br/>

### 장단기 메모리(LSTM)
Sepp Hochreiter 씨와 Juergen Schmidhuber 씨가 기울기 소실 문제의 해법으로 선보인 인기 있는 RNN 아키텍처 중 하나입니다. 이들은 연구 논문(PDF, 388KB)(IBM 외부 링크)에서 장기 종속성의 문제를 다룹니다. 즉, 현재 예측에 영향을 미치고 있는 이전 상태가 최근 과거가 아닐 경우, 이 RNN 모델에서 현재 상태를 정확히 예측하지 못할 수도 있습니다. 이를테면 “Alice is allergic to nuts. She can’t eat peanut butter.”에서 이탤릭체로 표시된 단어를 예측하고 싶다고 가정해 보겠습니다. 너트 알레르기라는 컨텍스트는 섭취 불가 식품이 너트를 함유하고 있음을 예측하는 데 도움이 될 것입니다. 하지만 이 컨텍스트가 몇 개의 문장 이전에 있다면, RNN에서 그 정보를 연결하는 게 어렵거나 아예 불가능합니다. LSTM에는 이러한 문제를 해결하기 위해 신경망의 숨은 계층에 “셀(cell)”이 있습니다. 이 셀은 3개의 게이트, 즉 입력(input) 게이트, 출력(output) 게이트, 망각(forget) 게이트가 있습니다. 이러한 게이트에서는 신경망에서 출력을 예측하는 데 필요한 정보의 흐름을 제어합니다.  예컨대 “she”와 같은 성별 대명사가 이전 문장에서 여러 차례 반복된 경우, 셀 상태에서 이를 제외할 수 있습니다.  

장단기 메모리(LSTM)는 모델이 더 긴 타임라인을 수용하도록 메모리 용량을 확장할 수 있는 RNN 변형입니다. RNN은 바로 이전의 입력만 기억할 수 있습니다. 따라서 여러 이전 시퀀스의 입력을 사용하여 예측을 개선할 수 없습니다.  

<br/>

### 게이트 순환 유닛(GRU)
이 RNN 변종도 RNN 모델의 단기 메모리 문제를 다룬다는 점에서 LSTM과 비슷합니다. 여기서는 "셀 상태"로 정보를 규제하는 게 아니라 숨은 상태를 사용합니다. 그리고 게이트가 3개가 아니라 2개입니다. 재설정(reset) 게이트와 업데이트(update) 게이트입니다. LSTM의 게이트와 비슷하게, 재설정 게이트와 업데이트 게이트는 어떤 정보를 얼마나 많이 보존할지를 제어합니다.  

게이트 순환 유닛(GRU)은 선택적 메모리 보존을 지원하는 RNN입니다. 이 모델은 업데이트를 추가하고, 메모리에서 정보를 저장하거나 제거할 수 있는 숨겨진 계층의 게이트를 망각합니다.   

